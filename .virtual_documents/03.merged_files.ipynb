import boto3, json, os
import pandas as pd
from botocore.config import Config


S3_BUCKET = "hab-ree-data-json"
S3_PREFIX = "demanda-real"
OUTPUT_KEY = "merged/ree-data_full.json"


with open("credentials/aws_keys.json") as f:
    keys = json.load(f)

os.environ["AWS_ACCESS_KEY_ID"] = keys["AWS_ACCESS_KEY_ID"]
os.environ["AWS_SECRET_ACCESS_KEY"] = keys["AWS_SECRET_ACCESS_KEY"]
os.environ["AWS_DEFAULT_REGION"] = keys["AWS_DEFAULT_REGION"]


s3 = boto3.client("s3")


# Bloque test robusto
dfs = []
processed = 0
MAX_FILES = 5  # número de archivos de prueba

paginator = s3.get_paginator("list_objects_v2")

for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=S3_PREFIX):
    contents = page.get("Contents", []) or []
    for obj in contents:
        key = obj["Key"]
        size = obj.get("Size", 0)

        # saltar "carpetas" y no-json
        if size == 0:
            print(f"[skip] {key} (size=0, prefijo/carpeta)")
            continue
        if not key.endswith(".json"):
            print(f"[skip] {key} (no .json)")
            continue

        try:
            resp = s3.get_object(Bucket=S3_BUCKET, Key=key)
            body = resp["Body"].read()  # bytes
            data = json.loads(body)     # admite bytes o str

            # Soporta dict con indicator.values o lista directa
            values = data.get("indicator", {}).get("values", []) if isinstance(data, dict) else data
            if not isinstance(values, list):
                print(f"[skip] {key} ('values' no es lista)")
                continue

            df = pd.DataFrame(values)
            dfs.append(df)
            processed += 1
            print(f"[{processed}] {key} -> {len(df)} filas")

        except json.JSONDecodeError as e:
            print(f"[error-json] {key}: {e}")
        except Exception as e:
            print(f"[error] {key}: {e}")

        if processed >= MAX_FILES:
            break
    if processed >= MAX_FILES:
        break

# Si quieres ver un preview:
if dfs:
    df_sample = pd.concat(dfs, ignore_index=True)
    print("Preview columnas:", list(df_sample.columns))
    print(df_sample.head())
else:
    print("No se procesó ningún .json con datos.")



# Bloque test
if dfs:
    sample_df = pd.concat(dfs, ignore_index=True)
    sample_df.head(10)

print(dfs)


dfs = []
paginator = s3.get_paginator("list_objects_v2")

for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=S3_PREFIX):
    for obj in page.get("Contents", []) or []:
        key = obj["Key"]
        size = obj.get("Size", 0)

        # 1) Saltar “carpetas” y no-json
        if size == 0 or not key.endswith(".json"):
            continue

        try:
            body = s3.get_object(Bucket=S3_BUCKET, Key=key)["Body"].read()
            data = json.loads(body)  # admite bytes

            # 2) Soporta dict con wrapper o lista directa
            values = data.get("indicator", {}).get("values", []) if isinstance(data, dict) else data
            if not isinstance(values, list) or not values:
                # nada que unir en este archivo
                continue

            dfs.append(pd.DataFrame(values))

        except json.JSONDecodeError as e:
            print(f"[error-json] {key}: {e}")
        except Exception as e:
            print(f"[error] {key}: {e}")

# 3) Unir, ordenar por fecha (si existe) y deduplicar
if dfs:
    df_total = pd.concat(dfs, ignore_index=True)

    # Detecta la columna temporal (datetime, datetime_utc, tz_time)
    time_col = next((c for c in ["datetime", "datetime_utc", "tz_time"] if c in df_total.columns), None)
    if time_col:
        # Parseo tolerante: si falla, seguimos sin ordenar
        try:
            df_total[time_col] = pd.to_datetime(df_total[time_col], errors="coerce")
            df_total = df_total.dropna(subset=[time_col]).sort_values(time_col).drop_duplicates(subset=[time_col])
        except Exception as e:
            print(f"[warn] No se pudo ordenar/deduplicar por {time_col}: {e}")
else:
    df_total = pd.DataFrame()

# df_total contiene el combinado final
print(f"Filas combinadas: {len(df_total)}")



dfs = []
paginator = s3.get_paginator("list_objects_v2")

for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=S3_PREFIX):
    for obj in page.get("Contents", []):
        key = obj["Key"] 

        body = s3.get_object(Bucket=S3_BUCKET, Key=key)["Body"].read()
        data = json.loads(body)

        values = data["indicator"]["values"] if isinstance(data, dict) else data
        dfs.append(pd.DataFrame(values))


full_df = pd.concat(dfs, ignore_index=True)
print("Filas totales:", len(full_df))


# Probar también con orient="split" https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html

payload = full_df.to_json(orient="records")
s3.put_object(
    Bucket=S3_BUCKET,
    Key=OUTPUT_KEY,
    Body=payload,
    ContentType="application/json"
)

print(f"Merged listo: s3://{S3_BUCKET}/{OUTPUT_KEY}")



